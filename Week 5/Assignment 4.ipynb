{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88858ea-2cff-4231-807f-f71c1469d0a2",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089a3cf-ac5f-484b-b79d-7ac05618e130",
   "metadata": {},
   "source": [
    "## Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d37701-e419-4aa4-a084-d9f9cea4e630",
   "metadata": {},
   "source": [
    "We are only using 10% of the space in one dimension, so the fraction of the available observations will be used to make a prediction is  $0.1 = \\frac1{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef02fb1-a17f-4111-8eab-73d6b208ab44",
   "metadata": {},
   "source": [
    "## Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2110a55-5107-477a-bd69-82cdb91e5fa3",
   "metadata": {},
   "source": [
    "We are only using 10% of the space in each dimension, so the fraction of the available observations will be used to make a prediction is  $0.1 \\cdot 0.1 = (0.1)^2 = \\frac1{100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75020db-5a17-40c9-83b4-5aea31ddd5df",
   "metadata": {},
   "source": [
    "## Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374ea17-2a55-4400-940f-c8f4fc7f4f77",
   "metadata": {},
   "source": [
    "Again, we are only using 10% of the space in each dimension, so the fraction of the available observations will be used to make a prediction is  $(0.1)^{100} = \\frac1{10^{100}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3572826-b1f9-4851-af74-a9eb2dc3d02c",
   "metadata": {},
   "source": [
    "## Part (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff10850-ac64-42a0-8266-318c81079746",
   "metadata": {},
   "source": [
    "As we can see, as the number of features and dimensions increase in the KNN model, a smaller fraction of the total space is used to make a prediction - and this fraction decreases exponentially. Therefore with a large $p$, there are very few training observations that are near $X$ which is a big drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee6a60-f795-4204-835b-6eb510cece2a",
   "metadata": {},
   "source": [
    "## Part (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235694f-07fa-4c99-bc8e-0ffdea10af4d",
   "metadata": {},
   "source": [
    "We need the prediction hypervolume to be 10% of the total hypervolume which is 1. If the side length of the prediction hypercube is $s$, then we see that $s^p = 0.1$ so $s = \\sqrt[p]{0.1}$\n",
    "\n",
    "For $p = 1$, $s = 0.1$\n",
    "\n",
    "For $p = 2$, $s \\approx 0.32$\n",
    "\n",
    "For $p = 100$, $s \\approx 0.98$\n",
    "\n",
    "This shows that as $p$ increases, $s$ approaches 1, meaning that to even search 10% of the overall you would have to search almost (but not quite) 100% of the feature space for each dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
